services:
  # MinIO - S3-compatible object storage
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password123
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    networks:
      - lakehouse-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 10s
      timeout: 5s
      retries: 5

  # Create MinIO buckets on startup
  minio-setup:
    image: minio/mc:latest
    container_name: minio-setup
    restart: "no"
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: /bin/sh -c "mc alias set myminio http://minio:9000 admin password123 && mc mb myminio/warehouse --ignore-existing && mc mb myminio/bronze --ignore-existing && mc mb myminio/silver --ignore-existing && mc mb myminio/gold --ignore-existing && echo Done"
    networks:
      - lakehouse-network

  # Nessie - Git-like catalog for Iceberg
  nessie:
    image: ghcr.io/projectnessie/nessie:latest
    container_name: nessie
    ports:
      - "19120:19120"
    environment:
      - NESSIE_VERSION_STORE_TYPE=ROCKSDB
      - NESSIE_ROCKSDB_STORE_PATH=/data/nessie
    volumes:
      - nessie-data:/data/nessie
    networks:
      - lakehouse-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:19120/api/v2/config" ]
      interval: 10s
      timeout: 10s
      retries: 5

  # Trino Coordinator
  trino:
    image: trinodb/trino:latest
    container_name: trino
    depends_on:
      nessie:
        condition: service_healthy
      minio:
        condition: service_healthy
    ports:
      - "8080:8080"
    volumes:
      - ./conf/trino/catalog:/etc/trino/catalog
      - ./conf/trino/config.properties:/etc/trino/config.properties
      - ./conf/trino/jvm_single.config:/etc/trino/jvm.config
    networks:
      - lakehouse-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/v1/info" ]
      interval: 10s
      timeout: 10s
      retries: 10

  # Download JARs first - no dependencies
  jar-downloader:
    image: curlimages/curl:latest
    container_name: jar-downloader
    user: root
    command: >
      /bin/sh -c "
      if [ ! -f /jars/iceberg-spark-runtime-4.0_2.13-1.10.0.jar ]; then
        echo '=== Downloading JARs ===' &&
        mkdir -p /jars &&
        cd /jars &&
        curl -LO https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-4.0_2.13/1.10.0/iceberg-spark-runtime-4.0_2.13-1.10.0.jar &&
        curl -LO https://repo1.maven.org/maven2/org/projectnessie/nessie-integrations/nessie-spark-extensions-3.5_2.13/0.99.0/nessie-spark-extensions-3.5_2.13-0.99.0.jar &&
        curl -LO https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.39.6/bundle-2.39.6.jar &&
        curl -LO https://repo1.maven.org/maven2/software/amazon/awssdk/url-connection-client/2.39.6/url-connection-client-2.39.6.jar &&
        curl -LO https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.1/hadoop-aws-3.4.1.jar &&
        touch /jars/.download-complete &&
        echo '=== JARs downloaded ==='
      else
        echo '=== JARs already present ==='
      fi"
    volumes:
      - spark-jars:/jars
    networks:
      - lakehouse-network

  # Spark Master
  spark-master:
    image: apache/spark:4.0.1
    container_name: spark-master
    user: root
    depends_on:
      jar-downloader:
        condition: service_completed_successfully
    ports:
      - "8081:8080"
      - "7077:7077"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_MASTER_HOST=spark-master
    command: >
      /bin/bash -c "
      echo '=== Copying JARs to master ===' &&
      cp /lakehouse-jars/*.jar /opt/spark/jars/ &&
      ls -lh /lakehouse-jars/ &&
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master --host spark-master --port 7077 --webui-port 8080"
    volumes:
      - spark-jars:/lakehouse-jars:ro
    networks:
      - lakehouse-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 10s
      timeout: 5s
      retries: 5

  # Spark Connect
  spark-connect:
    image: apache/spark:4.0.1
    container_name: spark-connect
    user: root
    depends_on:
      spark-master:
        condition: service_healthy
    ports:
      - "15002:15002"
    environment:
      - SPARK_NO_DAEMONIZE=true
    command: >
      /bin/bash -c "
      echo '=== Copying JARs ===' &&
      cp /lakehouse-jars/*.jar /opt/spark/jars/ &&
      echo '=== Starting Spark Connect Server ===' &&
      /opt/spark/sbin/start-connect-server.sh --master spark://spark-master:7077"
    volumes:
      - spark-jars:/lakehouse-jars:ro
    networks:
      - lakehouse-network

  # Workers - all depend on master and have JARs ready
  spark-worker-1:
    image: apache/spark:4.0.1
    container_name: spark-worker-1
    user: root
    depends_on:
      spark-master:
        condition: service_healthy
      jar-downloader:
        condition: service_completed_successfully
    ports:
      - "8082:8081"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
    command: >
      /bin/bash -c "
      echo '=== Copying JARs to worker-1 ===' &&
      cp /lakehouse-jars/*.jar /opt/spark/jars/ &&
      ls -lh /opt/spark/jars/ | grep -E 'iceberg|nessie|awssdk' &&
      echo '=== Starting Worker-1 ===' &&
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --webui-port 8081"
    volumes:
      - spark-jars:/lakehouse-jars:ro
    networks:
      - lakehouse-network

  spark-worker-2:
    image: apache/spark:4.0.1
    container_name: spark-worker-2
    user: root
    depends_on:
      spark-master:
        condition: service_healthy
      jar-downloader:
        condition: service_completed_successfully
    ports:
      - "8083:8081"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
    command: >
      /bin/bash -c "
      echo '=== Copying JARs to worker-2 ===' &&
      cp /lakehouse-jars/*.jar /opt/spark/jars/ &&
      echo '=== Starting Worker-2 ===' &&
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --webui-port 8081"
    volumes:
      - spark-jars:/lakehouse-jars:ro
    networks:
      - lakehouse-network

  spark-worker-3:
    image: apache/spark:4.0.1
    container_name: spark-worker-3
    user: root
    depends_on:
      spark-master:
        condition: service_healthy
      jar-downloader:
        condition: service_completed_successfully
    ports:
      - "8084:8081"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
    command: >
      /bin/bash -c "
      echo '=== Copying JARs to worker-3 ===' &&
      cp /lakehouse-jars/*.jar /opt/spark/jars/ &&
      echo '=== Starting Worker-3 ===' &&
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --webui-port 8081"
    volumes:
      - spark-jars:/lakehouse-jars:ro
    networks:
      - lakehouse-network

  spark-worker-4:
    image: apache/spark:4.0.1
    container_name: spark-worker-4
    user: root
    depends_on:
      spark-master:
        condition: service_healthy
      jar-downloader:
        condition: service_completed_successfully
    ports:
      - "8085:8081"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
    command: >
      /bin/bash -c "
      echo '=== Copying JARs to worker-4 ===' &&
      cp /lakehouse-jars/*.jar /opt/spark/jars/ &&
      echo '=== Starting Worker-4 ===' &&
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --webui-port 8081"
    volumes:
      - spark-jars:/lakehouse-jars:ro
    networks:
      - lakehouse-network

  # Zookeeper
  zookeeper:
    image: nexus.energia.sise:5003/bitnami/zookeeper:3.8
    container_name: zookeeper
    ports:
      - "2181:2181"
    volumes:
      - zookeeper-data:/bitnami
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    networks:
      - lakehouse-network

  # Kafka
  kafka:
    image: nexus.energia.sise:5003/bitnami/kafka:3.3
    container_name: kafka
    ports:
      - "9092:9092"
    volumes:
      - kafka-data:/bitnami
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://127.0.0.1:9092
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_ENABLE_KRAFT=no
      - KAFKA_CFG_NUM_PARTITIONS=6
    depends_on:
      - zookeeper
    networks:
      - lakehouse-network

volumes:
  minio-data:
  nessie-data:
  zookeeper-data:
  kafka-data:
  spark-jars:

networks:
  lakehouse-network:
    driver: bridge